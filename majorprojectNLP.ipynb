{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTdkQgxjl-zB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(url, num_pages):\n",
        "    reviews = []\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    for page in range(num_pages):\n",
        "        response = requests.get(f\"{url}?start={page*10}\", headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for review in soup.find_all('div', class_='text show-more__control'):\n",
        "            body = review.text.strip()\n",
        "            reviews.append({'body': body})\n",
        "\n",
        "    return reviews\n",
        "# List of movie URLs and their titles\n",
        "movies = [\n",
        "    {'url': 'https://www.imdb.com/title/tt2560140/reviews?ref_=tt_urv', 'title': 'Attack on Titan'},\n",
        "    {'url': 'https://www.imdb.com/title/tt0944947/reviews?ref_=tt_urv', 'title': 'Game of Thrones'},\n",
        "    # Add more movies as needed\n",
        "]\n",
        "\n",
        "num_pages = 5  # Adjust the number of pages to scrape\n",
        "\n",
        "with open('imdb_reviews.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['title', 'body']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for movie in movies:\n",
        "        reviews = scrape_imdb_reviews(movie['url'], num_pages)\n",
        "        for review in reviews:\n",
        "            review['title'] = movie['title']\n",
        "            writer.writerow(review)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('imdb_reviews_Series.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOL5s3EImJwh",
        "outputId": "aef4c94d-38ee-4db3-a166-883faeb20a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             title                                               body\n",
            "0  Attack on Titan  Attack On Titan is officially over. And nothin...\n",
            "1  Attack on Titan  The moment you watch this audiovisual masterpi...\n",
            "2  Attack on Titan  Before I started watching this show, I couldn'...\n",
            "3  Attack on Titan  I'm a more frequent American TV shows watcher ...\n",
            "4  Attack on Titan  Growing up in the 80's 90's during the time of...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    # Join tokens back into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['cleaned_body'] = df['body'].apply(preprocess_text)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tUN7a6MmOBN",
        "outputId": "8f03ead1-2c12-42b6-b9f5-f909bc2e94c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             title                                               body  \\\n",
            "0  Attack on Titan  Attack On Titan is officially over. And nothin...   \n",
            "1  Attack on Titan  The moment you watch this audiovisual masterpi...   \n",
            "2  Attack on Titan  Before I started watching this show, I couldn'...   \n",
            "3  Attack on Titan  I'm a more frequent American TV shows watcher ...   \n",
            "4  Attack on Titan  Growing up in the 80's 90's during the time of...   \n",
            "\n",
            "                                        cleaned_body  \n",
            "0  attack titan officially nothing ever samewhen ...  \n",
            "1  moment watch audiovisual masterpiece immediate...  \n",
            "2  started watching show couldnt imagine rated hi...  \n",
            "3  im frequent american tv shows watcher anime wa...  \n",
            "4  growing time top anime stories always getting ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    # Determine the sentiment\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif analysis.sentiment.polarity == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "df['sentiment'] = df['cleaned_body'].apply(analyze_sentiment)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiKX0uUcmRMQ",
        "outputId": "1c85aa96-b608-4ffb-d0df-96d8a422f933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             title                                               body  \\\n",
            "0  Attack on Titan  Attack On Titan is officially over. And nothin...   \n",
            "1  Attack on Titan  The moment you watch this audiovisual masterpi...   \n",
            "2  Attack on Titan  Before I started watching this show, I couldn'...   \n",
            "3  Attack on Titan  I'm a more frequent American TV shows watcher ...   \n",
            "4  Attack on Titan  Growing up in the 80's 90's during the time of...   \n",
            "\n",
            "                                        cleaned_body sentiment  \n",
            "0  attack titan officially nothing ever samewhen ...  Negative  \n",
            "1  moment watch audiovisual masterpiece immediate...  Positive  \n",
            "2  started watching show couldnt imagine rated hi...  Positive  \n",
            "3  im frequent american tv shows watcher anime wa...  Positive  \n",
            "4  growing time top anime stories always getting ...  Positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "print(df[['title','body', 'sentiment']])\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "df.to_csv('imdb_reviews_with_sentiments_Series.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFllOvm0mTtd",
        "outputId": "dcf32d5e-1651-42ba-9c02-f64da1d865f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               title                                               body  \\\n",
            "0    Attack on Titan  Attack On Titan is officially over. And nothin...   \n",
            "1    Attack on Titan  The moment you watch this audiovisual masterpi...   \n",
            "2    Attack on Titan  Before I started watching this show, I couldn'...   \n",
            "3    Attack on Titan  I'm a more frequent American TV shows watcher ...   \n",
            "4    Attack on Titan  Growing up in the 80's 90's during the time of...   \n",
            "..               ...                                                ...   \n",
            "365  Game of Thrones  Readers who come to this review later in the s...   \n",
            "366  Game of Thrones  Started off as the greatest series of all time...   \n",
            "367  Game of Thrones  I was a big fan of Game of Thrones ever since ...   \n",
            "368  Game of Thrones  Do not believe any of those negative reviews. ...   \n",
            "369  Game of Thrones  A series like never seen before which rocked l...   \n",
            "\n",
            "    sentiment  \n",
            "0    Negative  \n",
            "1    Positive  \n",
            "2    Positive  \n",
            "3    Positive  \n",
            "4    Positive  \n",
            "..        ...  \n",
            "365  Positive  \n",
            "366   Neutral  \n",
            "367  Positive  \n",
            "368  Positive  \n",
            "369   Neutral  \n",
            "\n",
            "[370 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from textblob import TextBlob\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Load the preprocessed data with sentiment analysis by TextBlob\n",
        "df = pd.read_csv('imdb_reviews_with_sentiments_Series.csv')\n",
        "# Remove neutral sentiments for binary classification\n",
        "df = df[df['sentiment'] != 'Neutral']\n",
        "# Map sentiment to binary labels\n",
        "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0})\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_body'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define models to compare\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Support Vector Machine\": SVC(kernel='linear')\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    # Predict the sentiment of the test set\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": report,\n",
        "        \"confusion_matrix\": confusion\n",
        "    }\n",
        "\n",
        "# Print the results for each model\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(metrics['classification_report'])\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(metrics['confusion_matrix'])\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Compare with TextBlob results\n",
        "textblob_sentiments = X_test.apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
        "textblob_accuracy = accuracy_score(y_test, textblob_sentiments)\n",
        "\n",
        "print(\"TextBlob Accuracy:\", textblob_accuracy)"
      ],
      "metadata": {
        "id": "NQPRooY0u9mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d990db8a-2597-47d0-982a-76f9909a821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Naive Bayes\n",
            "Accuracy: 0.971830985915493\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.60      0.75         5\n",
            "           1       0.97      1.00      0.99        66\n",
            "\n",
            "    accuracy                           0.97        71\n",
            "   macro avg       0.99      0.80      0.87        71\n",
            "weighted avg       0.97      0.97      0.97        71\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 3  2]\n",
            " [ 0 66]]\n",
            "\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.9295774647887324\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         5\n",
            "           1       0.93      1.00      0.96        66\n",
            "\n",
            "    accuracy                           0.93        71\n",
            "   macro avg       0.46      0.50      0.48        71\n",
            "weighted avg       0.86      0.93      0.90        71\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0  5]\n",
            " [ 0 66]]\n",
            "\n",
            "\n",
            "Model: Support Vector Machine\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         5\n",
            "           1       1.00      1.00      1.00        66\n",
            "\n",
            "    accuracy                           1.00        71\n",
            "   macro avg       1.00      1.00      1.00        71\n",
            "weighted avg       1.00      1.00      1.00        71\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 5  0]\n",
            " [ 0 66]]\n",
            "\n",
            "\n",
            "TextBlob Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the preprocessed data with sentiment analysis by TextBlob\n",
        "df = pd.read_csv('imdb_reviews_with_sentiments.csv')\n",
        "\n",
        "# Remove neutral sentiments for binary classification\n",
        "df = df[df['sentiment'] != 'Neutral']\n",
        "\n",
        "# Map sentiment to binary labels\n",
        "df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Negative': 0})\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_body'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define models to compare\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Support Vector Machine\": SVC(kernel='linear')\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    # Predict the sentiment of the test set\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"classification_report\": report,\n",
        "        \"confusion_matrix\": confusion\n",
        "    }\n",
        "\n",
        "# Print the results for each model\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(metrics['classification_report'])\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(metrics['confusion_matrix'])\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Compare with TextBlob results\n",
        "textblob_sentiments = X_test.apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
        "textblob_accuracy = accuracy_score(y_test, textblob_sentiments)\n",
        "\n",
        "print(\"TextBlob Accuracy:\", textblob_accuracy)"
      ],
      "metadata": {
        "id": "qQku9wopbOGS",
        "outputId": "a4730bbf-2f39-4cb5-c340-741f8a45f61e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Naive Bayes\n",
            "Accuracy: 0.8913043478260869\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.17      0.29         6\n",
            "           1       0.89      1.00      0.94        40\n",
            "\n",
            "    accuracy                           0.89        46\n",
            "   macro avg       0.94      0.58      0.61        46\n",
            "weighted avg       0.90      0.89      0.86        46\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1  5]\n",
            " [ 0 40]]\n",
            "\n",
            "\n",
            "Model: Logistic Regression\n",
            "Accuracy: 0.8695652173913043\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         6\n",
            "           1       0.87      1.00      0.93        40\n",
            "\n",
            "    accuracy                           0.87        46\n",
            "   macro avg       0.43      0.50      0.47        46\n",
            "weighted avg       0.76      0.87      0.81        46\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0  6]\n",
            " [ 0 40]]\n",
            "\n",
            "\n",
            "Model: Support Vector Machine\n",
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         6\n",
            "           1       1.00      1.00      1.00        40\n",
            "\n",
            "    accuracy                           1.00        46\n",
            "   macro avg       1.00      1.00      1.00        46\n",
            "weighted avg       1.00      1.00      1.00        46\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 6  0]\n",
            " [ 0 40]]\n",
            "\n",
            "\n",
            "TextBlob Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}